{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11075908,"sourceType":"datasetVersion","datasetId":6902859}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:50.301783Z","iopub.execute_input":"2025-03-20T18:14:50.302092Z","iopub.status.idle":"2025-03-20T18:14:50.614161Z","shell.execute_reply.started":"2025-03-20T18:14:50.302064Z","shell.execute_reply":"2025-03-20T18:14:50.613254Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cnnfiltered2/cnn_filtered2.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install required libraries\n!pip install transformers torch evaluate rouge-score pandas numpy tqdm nlpaug sacremoses nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:50.615219Z","iopub.execute_input":"2025-03-20T18:14:50.615578Z","iopub.status.idle":"2025-03-20T18:14:57.464007Z","shell.execute_reply.started":"2025-03-20T18:14:50.615556Z","shell.execute_reply":"2025-03-20T18:14:57.463195Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nCollecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=9909cda1e2b4abeb6b3384e0eb690bb39547a0e13314f6ff693b3fccc1ae39f6\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: sacremoses, rouge-score, nlpaug, evaluate\nSuccessfully installed evaluate-0.4.3 nlpaug-1.1.11 rouge-score-0.1.2 sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Imports\nimport torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom tqdm import tqdm\nfrom evaluate import load\nfrom sklearn.model_selection import train_test_split\nimport re\nimport os\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:57.465532Z","iopub.execute_input":"2025-03-20T18:14:57.465815Z","iopub.status.idle":"2025-03-20T18:15:18.194591Z","shell.execute_reply.started":"2025-03-20T18:14:57.465790Z","shell.execute_reply":"2025-03-20T18:15:18.193743Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 1. Load Model & Tokenizer\ncheckpoint_dir = \"/kaggle/working/best_bartlarge_model\"\ntokenizer_dir = \"/kaggle/working/best_bartlarge_tok\"\nif os.path.exists(checkpoint_dir) and os.path.exists(tokenizer_dir):\n    print(\"Loading from checkpoint (e.g., ROUGE-L = 0.305)...\")\n    model = BartForConditionalGeneration.from_pretrained(checkpoint_dir)\n    tokenizer = BartTokenizer.from_pretrained(tokenizer_dir)\nelse:\n    print(\"Starting from pretrained model...\")\n    model_name = \"facebook/bart-large-cnn\"\n    model = BartForConditionalGeneration.from_pretrained(model_name)\n    tokenizer = BartTokenizer.from_pretrained(model_name)\nmodel.config.dropout = 0.3\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:18.195788Z","iopub.execute_input":"2025-03-20T18:15:18.196307Z","iopub.status.idle":"2025-03-20T18:15:29.156444Z","shell.execute_reply.started":"2025-03-20T18:15:18.196282Z","shell.execute_reply":"2025-03-20T18:15:29.155678Z"}},"outputs":[{"name":"stdout","text":"Starting from pretrained model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d7a1ce8bc940019b7b445af6e5b741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed27ba542a3d4f18b90130cced18b5d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e46bbccb0ae411581a04dceefed8af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4096c2b1ad21463792facfa77934a6dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a663d23b08df4e7d8ab33a96189d39a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9bc3fdab87c45f5b39e50dffa877eee"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 2. Preprocessing & Augmentation\ndef preprocess_text(text):\n    text = str(text).lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:29.157328Z","iopub.execute_input":"2025-03-20T18:15:29.157641Z","iopub.status.idle":"2025-03-20T18:15:29.164649Z","shell.execute_reply.started":"2025-03-20T18:15:29.157611Z","shell.execute_reply":"2025-03-20T18:15:29.163716Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 3. Dataset Class\nclass NewsSummaryDataset(Dataset):\n    def __init__(self, data, tokenizer, max_input=512, max_output=160):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_input = max_input\n        self.max_output = max_output\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        article = preprocess_text(self.data.iloc[idx][\"article\"])\n        summary = preprocess_text(self.data.iloc[idx][\"highlights\"])\n\n        inputs = self.tokenizer(\n            article,\n            max_length=self.max_input,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        targets = self.tokenizer(\n            summary,\n            max_length=self.max_output,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n            \"labels\": targets[\"input_ids\"].squeeze(0)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:29.165555Z","iopub.execute_input":"2025-03-20T18:15:29.165860Z","iopub.status.idle":"2025-03-20T18:15:34.870874Z","shell.execute_reply.started":"2025-03-20T18:15:29.165827Z","shell.execute_reply":"2025-03-20T18:15:34.869988Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 4. Load Pure CNN Dataset (35K)\ndf = pd.read_csv(\"/kaggle/input/cnnfiltered2/cnn_filtered2.csv\")\ndf = df.dropna(subset=[\"article\", \"highlights\"])\n\nif len(df) < 35000:\n    print(f\"Warning: Dataset has {len(df)} samples, duplicating to 35,000...\")\n    df = pd.concat([df] * ((35000 // len(df)) + 1)).iloc[:35000]\nelse:\n    df = df.sample(n=35000, random_state=42)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)  # 28K train\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 3.5K val/test\nprint(f\"Dataset size: {len(df)} samples\")\nprint(f\"Train dataset size: {len(train_df)}\")\nprint(f\"Val dataset size: {len(val_df)}\")\nprint(f\"Test dataset size: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:34.871751Z","iopub.execute_input":"2025-03-20T18:15:34.872079Z","iopub.status.idle":"2025-03-20T18:15:38.781646Z","shell.execute_reply.started":"2025-03-20T18:15:34.872044Z","shell.execute_reply":"2025-03-20T18:15:38.780920Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 35000 samples\nTrain dataset size: 28000\nVal dataset size: 3500\nTest dataset size: 3500\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_dataset = NewsSummaryDataset(train_df, tokenizer)\nval_dataset = NewsSummaryDataset(val_df, tokenizer)\ntest_dataset = NewsSummaryDataset(test_df, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:38.783554Z","iopub.execute_input":"2025-03-20T18:15:38.783786Z","iopub.status.idle":"2025-03-20T18:15:38.787299Z","shell.execute_reply.started":"2025-03-20T18:15:38.783765Z","shell.execute_reply":"2025-03-20T18:15:38.786520Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 5. DataLoader\nBATCH_SIZE = 4\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:38.788338Z","iopub.execute_input":"2025-03-20T18:15:38.788624Z","iopub.status.idle":"2025-03-20T18:15:38.806516Z","shell.execute_reply.started":"2025-03-20T18:15:38.788596Z","shell.execute_reply":"2025-03-20T18:15:38.805890Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 6. Training Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nNUM_EPOCHS = 4\nTOTAL_STEPS = len(train_loader) * NUM_EPOCHS  # ~35,000 steps\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=TOTAL_STEPS)\nscaler = torch.cuda.amp.GradScaler()\naccumulation_steps = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:38.807259Z","iopub.execute_input":"2025-03-20T18:15:38.807472Z","iopub.status.idle":"2025-03-20T18:15:39.499128Z","shell.execute_reply.started":"2025-03-20T18:15:38.807452Z","shell.execute_reply":"2025-03-20T18:15:39.498284Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n<ipython-input-10-dee4d49dcb6d>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load ROUGE metric\nrouge = load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:15:39.499883Z","iopub.execute_input":"2025-03-20T18:15:39.500167Z","iopub.status.idle":"2025-03-20T18:15:40.569716Z","shell.execute_reply.started":"2025-03-20T18:15:39.500118Z","shell.execute_reply":"2025-03-20T18:15:40.568816Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b67b5b55834276aabe34b346270d5d"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# 7. Training Loop\nbest_rougeL = 0.0\npatience = 2\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_train_loss = 0\n    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\")):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = model(**batch)\n            loss = outputs.loss / accumulation_steps\n        scaler.scale(loss).backward()\n        if (step + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n        total_train_loss += loss.item() * accumulation_steps\n\n    model.eval()\n    total_val_loss = 0\n    val_summaries = []\n    val_references = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Val\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            total_val_loss += outputs.loss.item()\n            preds = model.generate(\n                batch[\"input_ids\"],\n                max_length=160,  # ~110 words\n                min_length=90,  # ~70 words\n                length_penalty=1.2,\n                num_beams=8,\n                early_stopping=True\n            )\n            summaries = tokenizer.batch_decode(preds, skip_special_tokens=True)\n            references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n            val_summaries.extend(summaries)\n            val_references.extend(references)\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    avg_val_loss = total_val_loss / len(val_loader)\n    rouge_scores = rouge.compute(predictions=val_summaries, references=val_references)\n    print(f\"Epoch {epoch+1}:\")\n    print(f\"  Train Loss: {avg_train_loss:.3f}\")\n    print(f\"  Val Loss: {avg_val_loss:.3f}\")\n    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.3f}, ROUGE-2: {rouge_scores['rouge2']:.3f}, ROUGE-L: {rouge_scores['rougeL']:.3f}\")\n\n    if rouge_scores['rougeL'] > best_rougeL:\n        best_rougeL = rouge_scores['rougeL']\n        model.save_pretrained(\"/kaggle/working/best_bartlarge_model\")\n        tokenizer.save_pretrained(\"/kaggle/working/best_bartlarge_tok\")\n        print(f\"Best model saved with ROUGE-L: {best_rougeL:.3f}\")\n    elif epoch > 1 and rouge_scores['rougeL'] < best_rougeL - 0.01:\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:16:06.515572Z","iopub.execute_input":"2025-03-20T18:16:06.515885Z","iopub.status.idle":"2025-03-21T04:33:25.749729Z","shell.execute_reply.started":"2025-03-20T18:16:06.515862Z","shell.execute_reply":"2025-03-21T04:33:25.748751Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 Train: 100%|██████████| 7000/7000 [1:15:13<00:00,  1.55it/s]\nEpoch 1 Val: 100%|██████████| 875/875 [1:21:30<00:00,  5.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1:\n  Train Loss: 1.285\n  Val Loss: 0.973\n  ROUGE-1: 0.469, ROUGE-2: 0.227, ROUGE-L: 0.309\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Best model saved with ROUGE-L: 0.309\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Train: 100%|██████████| 7000/7000 [1:15:03<00:00,  1.55it/s]\nEpoch 2 Val: 100%|██████████| 875/875 [1:18:53<00:00,  5.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2:\n  Train Loss: 0.927\n  Val Loss: 0.968\n  ROUGE-1: 0.469, ROUGE-2: 0.228, ROUGE-L: 0.310\nBest model saved with ROUGE-L: 0.310\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Train: 100%|██████████| 7000/7000 [1:15:09<00:00,  1.55it/s]\nEpoch 3 Val: 100%|██████████| 875/875 [1:14:49<00:00,  5.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3:\n  Train Loss: 0.827\n  Val Loss: 0.978\n  ROUGE-1: 0.470, ROUGE-2: 0.228, ROUGE-L: 0.311\nBest model saved with ROUGE-L: 0.311\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Train: 100%|██████████| 7000/7000 [1:15:14<00:00,  1.55it/s]\nEpoch 4 Val: 100%|██████████| 875/875 [1:19:42<00:00,  5.47s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4:\n  Train Loss: 0.740\n  Val Loss: 1.000\n  ROUGE-1: 0.474, ROUGE-2: 0.231, ROUGE-L: 0.312\nBest model saved with ROUGE-L: 0.312\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 8. Test Set Evaluation\nmodel.eval()\ntest_summaries = []\ntest_references = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Test Evaluation\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        preds = model.generate(\n            batch[\"input_ids\"],\n            max_length=160,\n            min_length=90,\n            length_penalty=1.2,\n            num_beams=8,\n            early_stopping=True\n        )\n        summaries = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n        test_summaries.extend(summaries)\n        test_references.extend(references)\n\ntest_rouge = rouge.compute(predictions=test_summaries, references=test_references)\nprint(\"Test Set Results:\")\nprint(f\"  ROUGE-1: {test_rouge['rouge1']:.3f}, ROUGE-2: {test_rouge['rouge2']:.3f}, ROUGE-L: {test_rouge['rougeL']:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T04:33:44.685729Z","iopub.execute_input":"2025-03-21T04:33:44.686046Z","iopub.status.idle":"2025-03-21T05:49:48.181744Z","shell.execute_reply.started":"2025-03-21T04:33:44.686019Z","shell.execute_reply":"2025-03-21T05:49:48.180917Z"}},"outputs":[{"name":"stderr","text":"Test Evaluation: 100%|██████████| 875/875 [1:15:42<00:00,  5.19s/it]\n","output_type":"stream"},{"name":"stdout","text":"Test Set Results:\n  ROUGE-1: 0.473, ROUGE-2: 0.230, ROUGE-L: 0.310\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# 9. Final Save and Zip\nmodel.save_pretrained(\"/kaggle/working/final_bartlarge_model\")\ntokenizer.save_pretrained(\"/kaggle/working/final_bartlarge_tok\")\nshutil.make_archive('/kaggle/working/best_bartlarge_model', 'zip', '/kaggle/working/best_bartlarge_model')\nshutil.make_archive('/kaggle/working/best_bartlarge_tok', 'zip', '/kaggle/working/best_bartlarge_tok')\nprint(\"Model and tokenizer zipped successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T05:50:06.362365Z","iopub.execute_input":"2025-03-21T05:50:06.362711Z","iopub.status.idle":"2025-03-21T05:51:31.183430Z","shell.execute_reply.started":"2025-03-21T05:50:06.362677Z","shell.execute_reply":"2025-03-21T05:51:31.182503Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer zipped successfully.\n","output_type":"stream"}],"execution_count":14}]}